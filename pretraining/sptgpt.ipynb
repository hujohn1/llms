{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import urllib.request\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x284fe8a24f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 123\n",
    "URL=(\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "filepath='the-verdict.txt'\n",
    "urllib.request.urlretrieve(URL, filepath)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, href):\n",
    "        self.href= href\n",
    "        self.rawtext=\"\"\n",
    "        self.mapping = []\n",
    "        self.inv_mapping = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.rawtext\n",
    "    \n",
    "    def tokenize(self):\n",
    "        if not self.mapping:\n",
    "            with open(self.href) as file:\n",
    "                self.rawtext = file.read()\n",
    "                result=re.split(r'([:;,.?!\"()]|--|\\s)', self.rawtext)\n",
    "                rstripped=map(lambda x: x.strip(), result)\n",
    "                final=[x for x in rstripped if x!='']\n",
    "                dictionary={word: idx for idx, word in enumerate(set(final))}\n",
    "                self.mapping = dictionary\n",
    "                result=[self.mapping[tk] for tk in final if tk in self.mapping]\n",
    "                self.inv_mapping = {self.mapping[key]: key for key in self.mapping}\n",
    "                return result\n",
    "\n",
    "    def encode(self, text):\n",
    "        result=re.split(r'([:;,.?!\"()]|--|\\s)', text)\n",
    "        rstripped=map(lambda x: x.strip(), result)\n",
    "        final=[x for x in rstripped if x!='']\n",
    "        result=[self.mapping[tk] for tk in final if tk in self.mapping]\n",
    "        return result\n",
    "\n",
    "    def decode(self, ids):\n",
    "        result = [self.inv_mapping[tk] for tk in ids if tk in self.inv_mapping]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', \"It's\", 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Encoder(filepath)\n",
    "tokenizer.tokenize()\n",
    "print(tokenizer.decode(tokenizer.encode(\"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.inputs=[]\n",
    "        self.preds=[]\n",
    "        enc=tokenizer.encode(text)\n",
    "        print(f'Encoding length {len(enc)}')\n",
    "\n",
    "        for i in range(0, len(enc)-max_length, stride):\n",
    "            self.inputs.append(torch.tensor(enc[i: i+max_length]))\n",
    "            self.preds.append(torch.tensor(enc[i+1: i+1+max_length]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.preds[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "#Byte pair Encoding\n",
    "ttokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "print(ttokenizer.decode(ttokenizer.encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoader(rawtext, ttokenizer, batch_size, max_length, stride):\n",
    "    dataset = GPTDataset(rawtext, ttokenizer, max_length, stride)\n",
    "    trainloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size= batch_size, \n",
    "        shuffle= False, \n",
    "        num_workers=0, \n",
    "        drop_last=True\n",
    "    )\n",
    "    return trainloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding length 5145\n",
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Parameter containing:\n",
      "tensor([[-0.3035, -0.5880,  0.3486,  ..., -0.0522, -1.0565,  1.1510],\n",
      "        [-1.3354, -2.9340,  0.1141,  ...,  0.9417, -0.3591,  0.0168],\n",
      "        [-0.1350, -0.5183,  0.2326,  ...,  0.5226,  0.5430,  1.8613],\n",
      "        ...,\n",
      "        [-1.0602,  0.2780, -2.7081,  ...,  2.1562, -0.2877, -0.6318],\n",
      "        [-1.0330,  0.2692, -0.8864,  ...,  0.5791, -0.6039, -1.0414],\n",
      "        [-1.0987,  0.2705,  0.2435,  ...,  0.4270,  0.3188, -0.8022]],\n",
      "       requires_grad=True)\n",
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "max_length=4\n",
    "dataloader = createDataLoader(tokenizer.rawtext, ttokenizer, 8, 4, max_length)\n",
    "data_iter=iter(dataloader)\n",
    "batch_one_inputs, batch_one_preds=next(data_iter)\n",
    "print(batch_one_inputs)\n",
    "\n",
    "vocab_size = 50527\n",
    "output_dim = 256\n",
    "\n",
    "tk_embedding_layer=torch.nn.Embedding(vocab_size, output_dim)\n",
    "#initial weights\n",
    "print(tk_embedding_layer.weight)\n",
    "#token level embeddings\n",
    "tk_embeddings = tk_embedding_layer(batch_one_inputs)\n",
    "print(tk_embeddings.shape)\n",
    "\n",
    "#Positional embeddings\n",
    "pos_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "#Input embeddings\n",
    "input_embeddings = pos_embeddings + tk_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\in {(batch\\_size, seq\\_len, dim\\_in)}$  \n",
    "$M_q, M_k, M_v \\in {(dim\\_in, dim\\_out)}$  \n",
    "$q, k, v \\in {(batch\\_size, seq\\_len, dim\\_out)}$  \n",
    "  \n",
    "$attn\\_scores, attn\\_weights \\in {(batch\\_size, seq\\_len, seq\\_len)}$  \n",
    "$cntxt\\_vec \\in {(batch\\_size, seq\\_len, dim\\_out)}$  \n",
    "\n",
    "For MHA  \n",
    "$q, k, v \\in {(batch\\_size, seq\\_len, num\\_heads, dim\\_out//num\\_heads)}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, seq_len, dropout):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        shape = (dim_in, dim_out)\n",
    "        self.M_k = nn.Parameter(torch.rand(shape))\n",
    "        self.M_q = nn.Parameter(torch.rand(shape))\n",
    "        self.M_v = nn.Parameter(torch.rand(shape))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = torch.tril(torch.ones((seq_len, seq_len)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        queries = X @ self.M_q\n",
    "        keys = X @ self.M_k\n",
    "        values = X @ self.M_v\n",
    "\n",
    "        #compute attention_scores\n",
    "        attn_scores=torch.softmax((queries @ keys.transpose(1,2))/keys.shape[-1]**0.5, dim=-1)\n",
    "        \n",
    "        #mask the attention_scores\n",
    "        masked_attn_scores = self.mask * attn_scores\n",
    "        #print(f\"MASKED ATTENTION \\n{masked_attn_scores}\")\n",
    "\n",
    "        #normalize attention weights\n",
    "        row_sums = masked_attn_scores.sum(dim=-1, keepdim=True)\n",
    "        normed_attn_weights = masked_attn_scores / row_sums\n",
    "        #print(f\"NORMED ATTENTION \\n{normed_attn_weights}\")\n",
    "\n",
    "        #apply dropout\n",
    "        normed_attn_weights = self.dropout(normed_attn_weights)\n",
    "\n",
    "        content_vec = normed_attn_weights @ values\n",
    "        return content_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, seq_len, dropout, num_heads):\n",
    "        super().__init__()\n",
    "        assert dim_out % num_heads ==0\n",
    "        self.heads = nn.ModuleList([SelfAttention(dim_in, dim_out, seq_len, dropout) for _ in range(num_heads)])\n",
    "    def forward(self, X):\n",
    "        return torch.cat([head(X) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, seq_len, dropout, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dim_out % num_heads ==0\n",
    "        shape=(dim_in, dim_out)\n",
    "        self.num_heads=num_heads\n",
    "        self.M_k = nn.Parameter(torch.rand(shape))\n",
    "        self.M_q = nn.Parameter(torch.rand(shape))\n",
    "        self.M_v = nn.Parameter(torch.rand(shape))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\n",
    "        self.out_proj = nn.Linear(dim_out, dim_out)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        queries = X @ self.M_q\n",
    "        keys = X @ self.M_k\n",
    "        values = X @ self.M_v\n",
    "\n",
    "        (batch_size, seq_len, dim_out) = queries.shape\n",
    "        queries=queries.view(batch_size, seq_len, self.num_heads, dim_out//self.num_heads)\n",
    "        keys=keys.view(batch_size, seq_len, self.num_heads, dim_out//self.num_heads)\n",
    "        values=values.view(batch_size, seq_len, self.num_heads, dim_out//self.num_heads)\n",
    "\n",
    "        queries=queries.transpose(1, 2)\n",
    "        keys=keys.transpose(1, 2)\n",
    "        values=values.transpose(1, 2)\n",
    "\n",
    "        attn_scores=queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores + self.mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        attn_weights=torch.softmax((attn_scores)/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "        \n",
    "        content_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        content_vec = content_vec.contiguous().view(batch_size, seq_len, dim_out)\n",
    "        content_vec = self.out_proj(content_vec)\n",
    "        return content_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5624, -1.4754],\n",
      "         [-0.6086, -1.5550],\n",
      "         [-0.6216, -1.5771],\n",
      "         [-0.5111, -1.4504],\n",
      "         [-0.4654, -1.3927],\n",
      "         [-0.4317, -1.3593]],\n",
      "\n",
      "        [[-0.5624, -1.4754],\n",
      "         [-0.6086, -1.5550],\n",
      "         [-0.6216, -1.5771],\n",
      "         [-0.5111, -1.4504],\n",
      "         [-0.4654, -1.3927],\n",
      "         [-0.4317, -1.3593]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "#print(batch.shape)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "ca = SelfAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "#print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\": 50527,\n",
    "    \"seq_len\": 1024, \n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1, \n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cnfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi)) * (x+0.044715 *torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cnfg, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cnfg[\"emb_dim\"], 4*cnfg[\"emb_dim\"]), \n",
    "            GeLU(), \n",
    "            nn.Linear( 4*cnfg[\"emb_dim\"],  cnfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_size, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(normalized_size))\n",
    "        self.shift = nn.Parameter(torch.ones(normalized_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        var = torch.var(x, dim=-1, keepdim=True, unbiased=False)\n",
    "        normed_x = (x-mean)/(var+self.eps)**0.5\n",
    "        return self.scale * normed_x + self.shift\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cnfg):\n",
    "        super().__init__()\n",
    "\n",
    "        #prelayer norm\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cnfg):\n",
    "        super().__init__()\n",
    "        self.tok_embed = torch.nn.Embedding(cnfg[\"vocab_size\"], cnfg[\"emb_dim\"])\n",
    "        self.pos_embed = torch.nn.Embedding(cnfg[\"seq_len\"], cnfg[\"emb_dim\"])\n",
    "        self.drop_embed = nn.Dropout(cnfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cnfg) for i in range(cnfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cnfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cnfg[\"emb_dim\"], cnfg['vocab_size'])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tk_embeddings=self.tok_embed(in_idx)\n",
    "        pos_embeddings = self.pos_embed(torch.arange(seq_len, device=in_idx.device))\n",
    "        embedding_sum = tk_embeddings + pos_embeddings\n",
    "        x = self.drop_embed(embedding_sum)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50527])\n",
      "tensor([[[-0.7788,  1.0670, -0.4077,  ...,  1.0887,  0.5278, -0.4219],\n",
      "         [-0.1521,  0.3581,  0.0937,  ...,  0.7276,  0.1815, -0.9721],\n",
      "         [-0.2564,  0.0832,  0.6153,  ...,  0.6698, -0.9098, -0.2385],\n",
      "         [ 0.7458,  0.0554,  0.8036,  ...,  0.5722, -0.0747,  0.0744]],\n",
      "\n",
      "        [[-1.0281,  0.7760, -0.2915,  ...,  0.5905,  0.4172, -0.1146],\n",
      "         [-0.6563, -0.2283,  0.3421,  ...,  1.1885,  0.0478,  0.0974],\n",
      "         [-0.5600, -1.0569,  0.2629,  ...,  0.7063, -0.1399, -0.7304],\n",
      "         [-0.0746, -0.5928,  0.6964,  ...,  1.7611,  0.2118, -1.3779]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Byte pair Encoding\n",
    "ttokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(ttokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(ttokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
